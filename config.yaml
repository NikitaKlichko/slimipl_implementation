# config/config.yaml

# Paths to data
train_paths:
  - "/path/to/train1"
  - "/path/to/train2"
val_paths:
  - "/path/to/val1"
  - "/path/to/val2"
unlabeled_path: "/path/to/unlabeled"
model_path: "/path/to/model_checkpoint"

# Batch size and number of workers
train_bs: 16
n_works: 4

# Hyperparameters for the model
learning_rate: 0.0002
supervised_updates: 10000
n_l_updates: 1
n_u_updates: 4
cache_size: 1000
cache_update_prob: 0.1
initial_dropout: 0.5
final_dropout: 0.1

# Optimizer settings
optimizer:
  type: "AdamW"
  lr: 2e-4
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.001

# Scheduler settings
scheduler:
  type: "ReduceLROnPlateau"
  mode: "min"
  patience: 10
  factor: 0.5

# Trainer specific settings
trainer:
  max_epochs: 100
  accumulate_grad_batches: 4
  check_val_every_n_epoch: 1
  log_every_n_steps: 5
  devices: 1
  accelerator: "gpu"
  precision: "bf16"
  enable_model_summary: true
  logger: "tensorboard"
  callbacks:
    - monitor: "val_wer"
      dirpath: "./ckpts/"
      filename: "best-checkpoint-{epoch:02d}-{val_wer:.3f}"
      save_last: true
      save_top_k: 1
      mode: "min"
      save_weights_only: true

# Concatenate multiple datasets (optional)
concat_probs: [0, 1]  # If not specified, each dataset is used separately.
